{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Client: 'tcp://127.0.0.1:38961' processes=4 threads=4, memory=8.35 GB>",
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:38961</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>4</li>\n  <li><b>Cores: </b>4</li>\n  <li><b>Memory: </b>8.35 GB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [0, 2, 4, 6] * 10, 'b': [1, 3, 5, 7] * 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0       0\n1       8\n2      64\n3     216\n4       0\n5       8\n6      64\n7     216\n8       0\n9       8\n10     64\n11    216\n12      0\n13      8\n14     64\n15    216\n16      0\n17      8\n18     64\n19    216\n20      0\n21      8\n22     64\n23    216\n24      0\n25      8\n26     64\n27    216\n28      0\n29      8\n30     64\n31    216\n32      0\n33      8\n34     64\n35    216\n36      0\n37      8\n38     64\n39    216\nName: a, dtype: int64"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df.a.apply(lambda x: x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dask DataFrame Structure:\n                   a      b\nnpartitions=4              \n0              int64  int64\n10               ...    ...\n20               ...    ...\n30               ...    ...\n39               ...    ...\nDask Name: from_pandas, 4 tasks",
      "text/html": "<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n    <tr>\n      <th>npartitions=4</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>int64</td>\n      <td>int64</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div>Dask Name: from_pandas, 4 tasks</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl = ddf.a.apply(lambda x: x**90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                                     0\n1                          1237940039285380274899124224\n2     1532495540865888858358347027150309183618739122...\n3     1080469556235987051829919370389914884872401572...\n4                                                     0\n5                          1237940039285380274899124224\n6     1532495540865888858358347027150309183618739122...\n7     1080469556235987051829919370389914884872401572...\n8                                                     0\n9                          1237940039285380274899124224\n10    1532495540865888858358347027150309183618739122...\n11    1080469556235987051829919370389914884872401572...\n12                                                    0\n13                         1237940039285380274899124224\n14    1532495540865888858358347027150309183618739122...\n15    1080469556235987051829919370389914884872401572...\n16                                                    0\n17                         1237940039285380274899124224\n18    1532495540865888858358347027150309183618739122...\n19    1080469556235987051829919370389914884872401572...\n20                                                    0\n21                         1237940039285380274899124224\n22    1532495540865888858358347027150309183618739122...\n23    1080469556235987051829919370389914884872401572...\n24                                                    0\n25                         1237940039285380274899124224\n26    1532495540865888858358347027150309183618739122...\n27    1080469556235987051829919370389914884872401572...\n28                                                    0\n29                         1237940039285380274899124224\n30    1532495540865888858358347027150309183618739122...\n31    1080469556235987051829919370389914884872401572...\n32                                                    0\n33                         1237940039285380274899124224\n34    1532495540865888858358347027150309183618739122...\n35    1080469556235987051829919370389914884872401572...\n36                                                    0\n37                         1237940039285380274899124224\n38    1532495540865888858358347027150309183618739122...\n39    1080469556235987051829919370389914884872401572...\nName: a, dtype: object"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "appl.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import momepy as mm\n",
    "import networkx as nx\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from consolidate import consolidate, roundabouts, topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x, y):\n",
    "    user = os.environ.get('DB_USER')\n",
    "    pwd = os.environ.get('DB_PWD')\n",
    "    host = os.environ.get('DB_HOST')\n",
    "    port = os.environ.get('DB_PORT')\n",
    "\n",
    "    url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\"\n",
    "    engine = create_engine(url)\n",
    "    # x, y = xy  # coordinates in epsg 27700\n",
    "    buffer = 1000  # radius in [m]\n",
    "\n",
    "    sql = f'SELECT * FROM openroads_200803_topological WHERE ST_DWithin(geometry, ST_SetSRID(ST_Point({x}, {y}), 27700), {buffer})'\n",
    "\n",
    "    df = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "\n",
    "    areas = [500, 2000]\n",
    "    compactness = [0.7, 0.9]\n",
    "    results = []\n",
    "\n",
    "    for area in areas:\n",
    "        for comp in compactness:\n",
    "            topo = consolidate(df, filter_func=roundabouts, area=area, circom=comp)\n",
    "\n",
    "            G = mm.gdf_to_nx(topo)\n",
    "            mesh = mm.meshedness(G, radius=None)\n",
    "            G = mm.subgraph(G, meshedness=True, cds_length=False, mean_node_degree=False, proportion={0: False, 3: False, 4: False}, cyclomatic=False, edge_node_ratio=False, gamma=False, local_closeness=True, closeness_weight=None, verbose=False)\n",
    "            vals = list(nx.get_node_attributes(G, 'meshedness').values())\n",
    "            l_mesh_mean = np.mean(vals)\n",
    "            l_mesh_median = np.median(vals)\n",
    "            l_mesh_dev = np.std(vals)\n",
    "            vals = list(nx.get_node_attributes(G, 'local_closeness').values())\n",
    "            l_close_mean = np.mean(vals)\n",
    "            l_close_median = np.median(vals)\n",
    "            l_close_dev = np.std(vals)\n",
    "            node_density = nx.number_of_nodes(G) / topo.length.sum()\n",
    "\n",
    "            results += [area, comp, mesh, l_mesh_mean, l_mesh_median, l_mesh_dev, l_close_mean, l_close_median, l_close_dev]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = pd.DataFrame([(334289.32, 390468.43), (413600.89, 130366.55), (355619.40, 145872.69), (253464.65, 62056.59)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas = pts.apply(lambda x:  check(x[0], x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(pts, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ddf.apply(lambda x:  check(x[0], x[1]), axis=1, meta=pd.Series(dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = vals.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    [500, 0.7, 0.181739879414298, 0.13087980656383...\n1    [500, 0.7, 0.07715582450832073, 0.052926181290...\n2    [500, 0.7, 0.052525252525252523, 0.04129909398...\n3    [500, 0.7, 0.0851063829787234, 0.0880821904608...\ndtype: object"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    [500, 0.7, 0.181739879414298, 0.13087980656383...\n1    [500, 0.7, 0.07715582450832073, 0.052926181290...\n2    [500, 0.7, 0.052525252525252523, 0.04129909398...\n3    [500, 0.7, 0.0851063829787234, 0.0880821904608...\ndtype: object"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_series_equal(computed, pas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_names = pd.read_csv('../opname_csv_gb/DATA/ST66.csv', index_col=None)\n",
    "header = pd.read_csv('../opname_csv_gb/DOC/OS_Open_Names_Header.csv', index_col=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [ID, NAMES_URI, NAME1, NAME1_LANG, NAME2, NAME2_LANG, TYPE, LOCAL_TYPE, GEOMETRY_X, GEOMETRY_Y, MOST_DETAIL_VIEW_RES, LEAST_DETAIL_VIEW_RES, MBR_XMIN, MBR_YMIN, MBR_XMAX, MBR_YMAX, POSTCODE_DISTRICT, POSTCODE_DISTRICT_URI, POPULATED_PLACE, POPULATED_PLACE_URI, POPULATED_PLACE_TYPE, DISTRICT_BOROUGH, DISTRICT_BOROUGH_URI, DISTRICT_BOROUGH_TYPE, COUNTY_UNITARY, COUNTY_UNITARY_URI, COUNTY_UNITARY_TYPE, REGION, REGION_URI, COUNTRY, COUNTRY_URI, RELATED_SPATIAL_OBJECT, SAME_AS_DBPEDIA, SAME_AS_GEONAMES]\nIndex: []\n\n[0 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>NAMES_URI</th>\n      <th>NAME1</th>\n      <th>NAME1_LANG</th>\n      <th>NAME2</th>\n      <th>NAME2_LANG</th>\n      <th>TYPE</th>\n      <th>LOCAL_TYPE</th>\n      <th>GEOMETRY_X</th>\n      <th>GEOMETRY_Y</th>\n      <th>...</th>\n      <th>COUNTY_UNITARY</th>\n      <th>COUNTY_UNITARY_URI</th>\n      <th>COUNTY_UNITARY_TYPE</th>\n      <th>REGION</th>\n      <th>REGION_URI</th>\n      <th>COUNTRY</th>\n      <th>COUNTRY_URI</th>\n      <th>RELATED_SPATIAL_OBJECT</th>\n      <th>SAME_AS_DBPEDIA</th>\n      <th>SAME_AS_GEONAMES</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_names.columns = header.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['Suburban Area', 'Village', 'Hamlet', 'City', 'Other Settlement',\n       'Town', 'Postcode', 'Named Road', 'Section Of Named Road',\n       'Section Of Numbered Road', 'Numbered Road', 'Hill Or Mountain',\n       'Woodland Or Forest', 'Inland Water', 'Other Landcover', 'Valley',\n       'Railway', 'Urban Greenspace', 'Spot Height', 'Cirque Or Hollow',\n       'Further Education', 'Primary Education', 'Secondary Education',\n       'Railway Station', 'Non State Secondary Education',\n       'Medical Care Accommodation', 'Special Needs Education',\n       'Further Education,Non State Secondary Education', 'Hospital',\n       'Higher or University Education', 'Non State Primary Education',\n       'Hospice', 'Bus Station', 'Electricity Production', 'Airfield',\n       'Non State Primary Education,Non State Secondary Education'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "open_names.LOCAL_TYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      ID                                          NAMES_URI  \\\n45  osgb4000000074572822  http://data.ordnancesurvey.co.uk/id/4000000074...   \n\n   NAME1  NAME1_LANG NAME2  NAME2_LANG            TYPE LOCAL_TYPE  GEOMETRY_X  \\\n45  Bath         NaN   NaN         NaN  populatedPlace       City      375093   \n\n    GEOMETRY_Y  ...                COUNTY_UNITARY  \\\n45      164923  ...  Bath and North East Somerset   \n\n                                   COUNTY_UNITARY_URI  \\\n45  http://data.ordnancesurvey.co.uk/id/7000000000...   \n\n                                  COUNTY_UNITARY_TYPE      REGION  \\\n45  http://data.ordnancesurvey.co.uk/ontology/admi...  South West   \n\n                                           REGION_URI  COUNTRY  \\\n45  http://data.ordnancesurvey.co.uk/id/7000000000...  England   \n\n                                          COUNTRY_URI RELATED_SPATIAL_OBJECT  \\\n45  http://data.ordnancesurvey.co.uk/id/country/en...                    NaN   \n\n   SAME_AS_DBPEDIA                 SAME_AS_GEONAMES  \n45             NaN  http://sws.geonames.org/2656173  \n\n[1 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>NAMES_URI</th>\n      <th>NAME1</th>\n      <th>NAME1_LANG</th>\n      <th>NAME2</th>\n      <th>NAME2_LANG</th>\n      <th>TYPE</th>\n      <th>LOCAL_TYPE</th>\n      <th>GEOMETRY_X</th>\n      <th>GEOMETRY_Y</th>\n      <th>...</th>\n      <th>COUNTY_UNITARY</th>\n      <th>COUNTY_UNITARY_URI</th>\n      <th>COUNTY_UNITARY_TYPE</th>\n      <th>REGION</th>\n      <th>REGION_URI</th>\n      <th>COUNTRY</th>\n      <th>COUNTRY_URI</th>\n      <th>RELATED_SPATIAL_OBJECT</th>\n      <th>SAME_AS_DBPEDIA</th>\n      <th>SAME_AS_GEONAMES</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45</th>\n      <td>osgb4000000074572822</td>\n      <td>http://data.ordnancesurvey.co.uk/id/4000000074...</td>\n      <td>Bath</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>populatedPlace</td>\n      <td>City</td>\n      <td>375093</td>\n      <td>164923</td>\n      <td>...</td>\n      <td>Bath and North East Somerset</td>\n      <td>http://data.ordnancesurvey.co.uk/id/7000000000...</td>\n      <td>http://data.ordnancesurvey.co.uk/ontology/admi...</td>\n      <td>South West</td>\n      <td>http://data.ordnancesurvey.co.uk/id/7000000000...</td>\n      <td>England</td>\n      <td>http://data.ordnancesurvey.co.uk/id/country/en...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>http://sws.geonames.org/2656173</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "open_names[open_names.LOCAL_TYPE == 'City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['NamedPlace']"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "fiona.listlayers('../opname_gpkg_gb/data/opname_gb.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "major = gpd.read_file('../Major_Towns_and_Cities__December_2015__Boundaries.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "major['geometry'] = major.centroid.to_crs(27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     objectid  tcity15cd      tcity15nm  st_areashape  st_lengthshape  \\\n0           1  J01000001       Barnsley  2.568247e+07   115099.860000   \n1           2  J01000002       Basildon  2.551499e+07   119299.838000   \n2           3  J01000003    Basingstoke  2.918502e+07    93900.388003   \n3           4  J01000004           Bath  2.423750e+07    92099.940000   \n4           5  J01000005        Bedford  2.016749e+07    71300.186000   \n..        ...        ...            ...           ...             ...   \n107       108  J01000108         Woking  2.908998e+07    92799.937100   \n108       109  J01000109  Wolverhampton  5.932501e+07   131300.020000   \n109       110  J01000110      Worcester  2.423248e+07    89800.048000   \n110       111  J01000111       Worthing  2.458249e+07    47000.022000   \n111       112  J01000112           York  3.369751e+07   114599.972002   \n\n                          geometry  \n0    POINT (434627.651 407669.833)  \n1    POINT (570873.671 189207.516)  \n2    POINT (463065.018 152019.027)  \n3    POINT (374795.202 164832.639)  \n4    POINT (505742.180 250327.575)  \n..                             ...  \n107  POINT (500684.336 159059.830)  \n108  POINT (391271.002 299238.180)  \n109  POINT (385584.581 255410.125)  \n110  POINT (512796.047 103944.437)  \n111  POINT (459940.884 452202.616)  \n\n[112 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>objectid</th>\n      <th>tcity15cd</th>\n      <th>tcity15nm</th>\n      <th>st_areashape</th>\n      <th>st_lengthshape</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>J01000001</td>\n      <td>Barnsley</td>\n      <td>2.568247e+07</td>\n      <td>115099.860000</td>\n      <td>POINT (434627.651 407669.833)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>J01000002</td>\n      <td>Basildon</td>\n      <td>2.551499e+07</td>\n      <td>119299.838000</td>\n      <td>POINT (570873.671 189207.516)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>J01000003</td>\n      <td>Basingstoke</td>\n      <td>2.918502e+07</td>\n      <td>93900.388003</td>\n      <td>POINT (463065.018 152019.027)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>J01000004</td>\n      <td>Bath</td>\n      <td>2.423750e+07</td>\n      <td>92099.940000</td>\n      <td>POINT (374795.202 164832.639)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>J01000005</td>\n      <td>Bedford</td>\n      <td>2.016749e+07</td>\n      <td>71300.186000</td>\n      <td>POINT (505742.180 250327.575)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>108</td>\n      <td>J01000108</td>\n      <td>Woking</td>\n      <td>2.908998e+07</td>\n      <td>92799.937100</td>\n      <td>POINT (500684.336 159059.830)</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>109</td>\n      <td>J01000109</td>\n      <td>Wolverhampton</td>\n      <td>5.932501e+07</td>\n      <td>131300.020000</td>\n      <td>POINT (391271.002 299238.180)</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>110</td>\n      <td>J01000110</td>\n      <td>Worcester</td>\n      <td>2.423248e+07</td>\n      <td>89800.048000</td>\n      <td>POINT (385584.581 255410.125)</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>111</td>\n      <td>J01000111</td>\n      <td>Worthing</td>\n      <td>2.458249e+07</td>\n      <td>47000.022000</td>\n      <td>POINT (512796.047 103944.437)</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>112</td>\n      <td>J01000112</td>\n      <td>York</td>\n      <td>3.369751e+07</td>\n      <td>114599.972002</td>\n      <td>POINT (459940.884 452202.616)</td>\n    </tr>\n  </tbody>\n</table>\n<p>112 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = pd.DataFrame(major.set_index('tcity15nm').geometry.apply(lambda x: (x.x, x.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               geometry\ntcity15nm                                              \nBarnsley          (434627.6513354856, 407669.832611784)\nBasildon         (570873.670668373, 189207.51646598754)\nBasingstoke      (463065.0179892903, 152019.0270463108)\nBath             (374795.2024792732, 164832.6389714771)\nBedford        (505742.17987574486, 250327.57510175492)\n...                                                 ...\nWoking         (500684.33641567733, 159059.83044815497)\nWolverhampton  (391271.00219945755, 299238.17997642065)\nWorcester       (385584.5812961166, 255410.12549394125)\nWorthing        (512796.0471801633, 103944.43660039204)\nYork           (459940.88361425925, 452202.61605940934)\n\n[112 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geometry</th>\n    </tr>\n    <tr>\n      <th>tcity15nm</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Barnsley</th>\n      <td>(434627.6513354856, 407669.832611784)</td>\n    </tr>\n    <tr>\n      <th>Basildon</th>\n      <td>(570873.670668373, 189207.51646598754)</td>\n    </tr>\n    <tr>\n      <th>Basingstoke</th>\n      <td>(463065.0179892903, 152019.0270463108)</td>\n    </tr>\n    <tr>\n      <th>Bath</th>\n      <td>(374795.2024792732, 164832.6389714771)</td>\n    </tr>\n    <tr>\n      <th>Bedford</th>\n      <td>(505742.17987574486, 250327.57510175492)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Woking</th>\n      <td>(500684.33641567733, 159059.83044815497)</td>\n    </tr>\n    <tr>\n      <th>Wolverhampton</th>\n      <td>(391271.00219945755, 299238.17997642065)</td>\n    </tr>\n    <tr>\n      <th>Worcester</th>\n      <td>(385584.5812961166, 255410.12549394125)</td>\n    </tr>\n    <tr>\n      <th>Worthing</th>\n      <td>(512796.0471801633, 103944.43660039204)</td>\n    </tr>\n    <tr>\n      <th>York</th>\n      <td>(459940.88361425925, 452202.61605940934)</td>\n    </tr>\n  </tbody>\n</table>\n<p>112 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.to_csv('major_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --network=\"host\"  --rm -ti -p 8786:8786 darribas/gds_dev:5.0 start.sh dask-worker 138.253.73.214:8786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import momepy as mm\n",
    "import networkx as nx\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# from data_processing.vector_data.consolidate import consolidate, roundabouts, topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(xy):\n",
    "    \n",
    "    from itertools import combinations\n",
    "    import collections\n",
    "\n",
    "    import pygeos\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import momepy as mm\n",
    "\n",
    "    from shapely.ops import polygonize\n",
    "    from scipy.spatial import Voronoi\n",
    "\n",
    "\n",
    "    # helper functions\n",
    "    def get_ids(x, ids):\n",
    "        return ids[x]\n",
    "\n",
    "\n",
    "    mp = np.vectorize(get_ids, excluded=[\"ids\"])\n",
    "\n",
    "\n",
    "    def dist(p1, p2):\n",
    "        return np.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))\n",
    "\n",
    "\n",
    "    def get_verts(x, voronoi_diagram):\n",
    "        return voronoi_diagram.vertices[x]\n",
    "\n",
    "\n",
    "    def _average_geometry(lines, poly=None, distance=2):\n",
    "        \"\"\"\n",
    "        Returns average geometry.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lines : list\n",
    "            LineStrings connected at endpoints forming a closed polygon\n",
    "        poly : shapely.geometry.Polygon\n",
    "            polygon enclosed by `lines`\n",
    "        distance : float\n",
    "            distance for interpolation\n",
    "\n",
    "        Returns list of averaged geometries\n",
    "        \"\"\"\n",
    "        if not poly:\n",
    "            polygons = list(polygonize(lines))\n",
    "            if len(polygons) == 1:\n",
    "                poly = polygons[0]\n",
    "            else:\n",
    "                raise ValueError(\"given lines do not form a single polygon\")\n",
    "        # get an additional line around the lines to avoid infinity issues with Voronoi\n",
    "        extended_lines = [poly.buffer(distance).exterior] + lines\n",
    "\n",
    "        # interpolate lines to represent them as points for Voronoi\n",
    "        points = np.empty((0, 2))\n",
    "        ids = []\n",
    "\n",
    "        pygeos_lines = pygeos.from_shapely(extended_lines)\n",
    "        lengths = pygeos.length(pygeos_lines)\n",
    "        for ix, (line, length) in enumerate(zip(pygeos_lines, lengths)):\n",
    "            pts = pygeos.line_interpolate_point(\n",
    "                line, np.linspace(0.1, length - 0.1, num=int((length - 0.1) // distance))\n",
    "            )  # .1 offset to keep a gap between two segments\n",
    "            points = np.append(points, pygeos.get_coordinates(pts), axis=0)\n",
    "            ids += [ix] * len(pts)\n",
    "\n",
    "            # here we might also want to append original coordinates of each line\n",
    "            # to get a higher precision on the corners, but it does not seem to be\n",
    "            # necessary based on my tests.\n",
    "\n",
    "        # generate Voronoi diagram\n",
    "        voronoi_diagram = Voronoi(points)\n",
    "\n",
    "        # get all rigdes and filter only those between the two lines\n",
    "        pts = voronoi_diagram.ridge_points\n",
    "        mapped = mp(pts, ids=ids)\n",
    "\n",
    "        # iterate over segment-pairs\n",
    "        edgelines = []\n",
    "        for a, b in combinations(range(1, len(lines) + 1), 2):\n",
    "            mask = (\n",
    "                np.isin(mapped[:, 0], [a, b])\n",
    "                & np.isin(mapped[:, 1], [a, b])\n",
    "                & (mapped[:, 0] != mapped[:, 1])\n",
    "            )\n",
    "            rigde_vertices = np.array(voronoi_diagram.ridge_vertices)\n",
    "            verts = rigde_vertices[mask]\n",
    "\n",
    "            # generate the line in between the lines\n",
    "            edgeline = pygeos.line_merge(\n",
    "                pygeos.multilinestrings(get_verts(verts, voronoi_diagram))\n",
    "            )\n",
    "            snapped = pygeos.snap(edgeline, pygeos_lines[a], distance)\n",
    "            edgelines.append(snapped)\n",
    "        return edgelines\n",
    "\n",
    "\n",
    "    def consolidate(network, distance=2, epsilon=2, filter_func=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Consolidate edges of a network, takes care of geometry only. No\n",
    "        attributes are preserved at the moment.\n",
    "\n",
    "        The whole process is split into several steps:\n",
    "        1. Polygonize network\n",
    "        2. Find polygons which are likely caused by dual lines and other\n",
    "           geometries to be consolidated.\n",
    "        3. Iterate over those polygons and generate averaged geometry\n",
    "        4. Remove invalid and merge together with new geometry.\n",
    "\n",
    "        Step 2 needs work, this is just a first attempt based on shape and area\n",
    "        of the polygon. We will have to come with clever options here and\n",
    "        allow their specification, because each network will need different\n",
    "        parameters.\n",
    "\n",
    "        Either before or after these steps needs to be done node consolidation,\n",
    "        but in a way which does not generate overlapping geometries.\n",
    "        Overlapping geometries cause (unresolvable) issues with Voronoi.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network : GeoDataFrame (LineStrings)\n",
    "\n",
    "        distance : float\n",
    "            distance for interpolation\n",
    "\n",
    "        epsilon : float\n",
    "            tolerance for simplification\n",
    "\n",
    "        filter_func : function\n",
    "            function which takes gdf of polygonized network and returns mask of invalid\n",
    "            polygons (those which should be consolidated)\n",
    "\n",
    "        **kwargs\n",
    "            Additional kwargs passed to filter_func\n",
    "        \"\"\"\n",
    "\n",
    "        # polygonize network\n",
    "        polygonized = polygonize(network.geometry)\n",
    "        geoms = [g for g in polygonized]\n",
    "        gdf = gpd.GeoDataFrame(geometry=geoms, crs=network.crs)\n",
    "\n",
    "        # filter potentially incorrect polygons\n",
    "        mask = filter_func(gdf, **kwargs)\n",
    "        invalid = gdf.loc[mask]\n",
    "\n",
    "        sindex = network.sindex\n",
    "\n",
    "        # iterate over polygons which are marked to be consolidated\n",
    "        # list segments to be removed and the averaged geoms replacing them\n",
    "        averaged = []\n",
    "        to_remove = []\n",
    "        for poly in invalid.geometry:\n",
    "            real = network.iloc[sindex.query(poly.exterior, predicate=\"intersects\")]\n",
    "            mask = real.intersection(poly.exterior).type.isin(\n",
    "                [\"LineString\", \"MultiLineString\"]\n",
    "            )\n",
    "            real = real[mask]\n",
    "            lines = list(real.geometry)\n",
    "            to_remove += list(real.index)\n",
    "\n",
    "            if lines:\n",
    "                av = _average_geometry(lines, poly, distance)\n",
    "                averaged += av\n",
    "\n",
    "        # drop double lines\n",
    "        clean = network.drop(set(to_remove))\n",
    "\n",
    "        # merge new geometries with the existing network\n",
    "        averaged = gpd.array.from_shapely(averaged, crs=network.crs).simplify(epsilon)\n",
    "        result = pd.concat([clean, gpd.GeoDataFrame(geometry=averaged[~averaged.is_empty])])\n",
    "        merge = topology(result)\n",
    "\n",
    "        return merge\n",
    "\n",
    "\n",
    "    def roundabouts(gdf, area=5000, circom=0.6):\n",
    "        \"\"\"\n",
    "        Filter out roundabouts\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate parameters\n",
    "        gdf[\"area\"] = gdf.geometry.area\n",
    "        gdf[\"circom\"] = mm.CircularCompactness(gdf, \"area\").series\n",
    "        # select valid and invalid network-net_blocks\n",
    "        mask = (gdf[\"area\"] < area) & (gdf[\"circom\"] > circom)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def filter_comp(gdf, max_size=10000, circom_max=0.2):\n",
    "        \"\"\"\n",
    "        Filter based on max size and compactness\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf : GeoDataFrame\n",
    "            polygonized network\n",
    "        max_size : float\n",
    "            maximum size of a polygon to be considered potentially invalid\n",
    "        circom_max : float\n",
    "            maximum circular compactness of a polygon to be considered\n",
    "            potentially invalid.\n",
    "\n",
    "        Returns boolean series\n",
    "\n",
    "        \"\"\"\n",
    "        # calculate parameters\n",
    "        gdf[\"area\"] = gdf.geometry.area\n",
    "        gdf[\"circom\"] = mm.CircularCompactness(gdf, \"area\").series\n",
    "        # select valid and invalid network-net_blocks\n",
    "        mask = (gdf[\"area\"] < max_size) & (gdf[\"circom\"] < circom_max)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def topology(gdf):\n",
    "        \"\"\"\n",
    "        Clean topology of existing LineString geometry by removal of nodes of degree 2.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf : GeoDataFrame\n",
    "            (Multi)LineString data of street network\n",
    "        \"\"\"\n",
    "\n",
    "        # explode to avoid MultiLineStrings\n",
    "        # double reset index due to the bug in GeoPandas explode\n",
    "        df = gdf.reset_index(drop=True).explode().reset_index(drop=True)\n",
    "\n",
    "        # get underlying pygeos geometry\n",
    "        geom = df.geometry.values.data\n",
    "\n",
    "        # extract array of coordinates and number per geometry\n",
    "        coords = pygeos.get_coordinates(geom)\n",
    "        indices = pygeos.get_num_coordinates(geom)\n",
    "\n",
    "        # generate a list of start and end coordinates and create point geometries\n",
    "        edges = [0]\n",
    "        i = 0\n",
    "        for ind in indices:\n",
    "            ix = i + ind\n",
    "            edges.append(ix - 1)\n",
    "            edges.append(ix)\n",
    "            i = ix\n",
    "        edges = edges[:-1]\n",
    "        points = pygeos.points(np.unique(coords[edges], axis=0))\n",
    "\n",
    "        # query LineString geometry to identify points intersecting 2 geometries\n",
    "        tree = pygeos.STRtree(geom)\n",
    "        inp, res = tree.query_bulk(points, predicate=\"intersects\")\n",
    "        unique, counts = np.unique(inp, return_counts=True)\n",
    "        merge = res[np.isin(inp, unique[counts == 2])]\n",
    "\n",
    "        # filter duplications and create a dictionary with indication of components to be merged together\n",
    "        dups = [item for item, count in collections.Counter(merge).items() if count > 1]\n",
    "        split = np.split(merge, len(merge) / 2)\n",
    "        components = {}\n",
    "        for i, a in enumerate(split):\n",
    "            if a[0] in dups or a[1] in dups:\n",
    "                if a[0] in components.keys():\n",
    "                    i = components[a[0]]\n",
    "                elif a[1] in components.keys():\n",
    "                    i = components[a[1]]\n",
    "            components[a[0]] = i\n",
    "            components[a[1]] = i\n",
    "\n",
    "        # iterate through components and create new geometries\n",
    "        new = []\n",
    "        for c in set(components.values()):\n",
    "            keys = []\n",
    "            for item in components.items():\n",
    "                if item[1] == c:\n",
    "                    keys.append(item[0])\n",
    "            new.append(pygeos.line_merge(pygeos.union_all(geom[keys])))\n",
    "\n",
    "        # remove incorrect geometries and append fixed versions\n",
    "        df = df.drop(merge)\n",
    "        final = gpd.GeoSeries(new).explode().reset_index(drop=True)\n",
    "        return df.append(\n",
    "            gpd.GeoDataFrame({df.geometry.name: final}, geometry=df.geometry.name),\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    user = 'martin'\n",
    "    pwd = 'gdsl2020'\n",
    "    host = '138.253.73.214'\n",
    "    port = '5432'\n",
    "\n",
    "    url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\"\n",
    "    engine = create_engine(url)\n",
    "    # x, y = xy  # coordinates in epsg 27700\n",
    "    buffer = 10000  # radius in [m]\n",
    "\n",
    "    sql = f'SELECT * FROM openroads_200803_topological WHERE ST_DWithin(geometry, ST_SetSRID(ST_Point({xy[0][1:-1]}), 27700), {buffer})'\n",
    "\n",
    "    df = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "\n",
    "#     areas = range(500, 2501, 500)\n",
    "#     compactness = np.linspace(0.7, 0.9, 5)\n",
    "    areas=[500]\n",
    "    compactness=[0.8]\n",
    "    results = []\n",
    "\n",
    "    for area in areas:\n",
    "        for comp in compactness:\n",
    "            topo = consolidate(df, filter_func=roundabouts, area=area, circom=comp)\n",
    "\n",
    "            G = mm.gdf_to_nx(topo)\n",
    "            mesh = mm.meshedness(G, radius=None)\n",
    "            G = mm.subgraph(G, meshedness=True, cds_length=False, mean_node_degree=False, proportion={0: False, 3: False, 4: False}, cyclomatic=False, edge_node_ratio=False, gamma=False, local_closeness=True, closeness_weight=None, verbose=False)\n",
    "            vals = list(nx.get_node_attributes(G, 'meshedness').values())\n",
    "            l_mesh_mean = np.mean(vals)\n",
    "            l_mesh_median = np.median(vals)\n",
    "            l_mesh_dev = np.std(vals)\n",
    "            vals = list(nx.get_node_attributes(G, 'local_closeness').values())\n",
    "            l_close_mean = np.mean(vals)\n",
    "            l_close_median = np.median(vals)\n",
    "            l_close_dev = np.std(vals)\n",
    "            node_density = nx.number_of_nodes(G) / topo.length.sum()\n",
    "\n",
    "            results += [area, comp, mesh, l_mesh_mean, l_mesh_median, l_mesh_dev, l_close_mean, l_close_median, l_close_dev]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('https://gist.githubusercontent.com/martinfleis/4148eeb0ea19e7808e761e097a877196/raw/f3001eeb7294eb330c6675d1cab307cd562baa5c/major_cities.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(cities.iloc[:4], npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ddf.apply(lambda x:  check(x), axis=1, meta=pd.Series(dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = vals.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tcity15nm\nBarnsley       [500, 0.8, 0.0897190163533298, 0.0637388607946...\nBasildon       [500, 0.8, 0.0757020757020757, 0.0516538411705...\nBasingstoke    [500, 0.8, 0.07523718467308965, 0.052268476211...\nBath           [500, 0.8, 0.08491286537526904, 0.057112326713...\ndtype: object"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}