{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(\"tcp://172.17.0.2:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8787/status' target='_blank'>http://172.17.0.2:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>28</li>\n",
       "  <li><b>Memory: </b>84.28 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:8786' processes=1 threads=28, memory=84.28 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import momepy as mm\n",
    "import networkx as nx\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(xy):\n",
    "    \n",
    "    from itertools import combinations\n",
    "    import collections\n",
    "\n",
    "    import pygeos\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import momepy as mm\n",
    "\n",
    "    from shapely.ops import polygonize\n",
    "    from scipy.spatial import Voronoi\n",
    "\n",
    "\n",
    "    # helper functions\n",
    "    def get_ids(x, ids):\n",
    "        return ids[x]\n",
    "\n",
    "\n",
    "    mp = np.vectorize(get_ids, excluded=[\"ids\"])\n",
    "\n",
    "\n",
    "    def dist(p1, p2):\n",
    "        return np.sqrt(((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2))\n",
    "\n",
    "\n",
    "    def get_verts(x, voronoi_diagram):\n",
    "        return voronoi_diagram.vertices[x]\n",
    "\n",
    "\n",
    "    def _average_geometry(lines, poly=None, distance=2):\n",
    "        \"\"\"\n",
    "        Returns average geometry.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lines : list\n",
    "            LineStrings connected at endpoints forming a closed polygon\n",
    "        poly : shapely.geometry.Polygon\n",
    "            polygon enclosed by `lines`\n",
    "        distance : float\n",
    "            distance for interpolation\n",
    "\n",
    "        Returns list of averaged geometries\n",
    "        \"\"\"\n",
    "        if not poly:\n",
    "            polygons = list(polygonize(lines))\n",
    "            if len(polygons) == 1:\n",
    "                poly = polygons[0]\n",
    "            else:\n",
    "                raise ValueError(\"given lines do not form a single polygon\")\n",
    "        # get an additional line around the lines to avoid infinity issues with Voronoi\n",
    "        extended_lines = [poly.buffer(distance).exterior] + lines\n",
    "\n",
    "        # interpolate lines to represent them as points for Voronoi\n",
    "        points = np.empty((0, 2))\n",
    "        ids = []\n",
    "\n",
    "        pygeos_lines = pygeos.from_shapely(extended_lines)\n",
    "        lengths = pygeos.length(pygeos_lines)\n",
    "        for ix, (line, length) in enumerate(zip(pygeos_lines, lengths)):\n",
    "            pts = pygeos.line_interpolate_point(\n",
    "                line, np.linspace(0.1, length - 0.1, num=int((length - 0.1) // distance))\n",
    "            )  # .1 offset to keep a gap between two segments\n",
    "            points = np.append(points, pygeos.get_coordinates(pts), axis=0)\n",
    "            ids += [ix] * len(pts)\n",
    "\n",
    "            # here we might also want to append original coordinates of each line\n",
    "            # to get a higher precision on the corners, but it does not seem to be\n",
    "            # necessary based on my tests.\n",
    "\n",
    "        # generate Voronoi diagram\n",
    "        voronoi_diagram = Voronoi(points)\n",
    "\n",
    "        # get all rigdes and filter only those between the two lines\n",
    "        pts = voronoi_diagram.ridge_points\n",
    "        mapped = mp(pts, ids=ids)\n",
    "\n",
    "        # iterate over segment-pairs\n",
    "        edgelines = []\n",
    "        for a, b in combinations(range(1, len(lines) + 1), 2):\n",
    "            mask = (\n",
    "                np.isin(mapped[:, 0], [a, b])\n",
    "                & np.isin(mapped[:, 1], [a, b])\n",
    "                & (mapped[:, 0] != mapped[:, 1])\n",
    "            )\n",
    "            rigde_vertices = np.array(voronoi_diagram.ridge_vertices)\n",
    "            verts = rigde_vertices[mask]\n",
    "\n",
    "            # generate the line in between the lines\n",
    "            edgeline = pygeos.line_merge(\n",
    "                pygeos.multilinestrings(get_verts(verts, voronoi_diagram))\n",
    "            )\n",
    "            snapped = pygeos.snap(edgeline, pygeos_lines[a], distance)\n",
    "            edgelines.append(snapped)\n",
    "        return edgelines\n",
    "\n",
    "\n",
    "    def consolidate(network, distance=2, epsilon=2, filter_func=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Consolidate edges of a network, takes care of geometry only. No\n",
    "        attributes are preserved at the moment.\n",
    "\n",
    "        The whole process is split into several steps:\n",
    "        1. Polygonize network\n",
    "        2. Find polygons which are likely caused by dual lines and other\n",
    "           geometries to be consolidated.\n",
    "        3. Iterate over those polygons and generate averaged geometry\n",
    "        4. Remove invalid and merge together with new geometry.\n",
    "\n",
    "        Step 2 needs work, this is just a first attempt based on shape and area\n",
    "        of the polygon. We will have to come with clever options here and\n",
    "        allow their specification, because each network will need different\n",
    "        parameters.\n",
    "\n",
    "        Either before or after these steps needs to be done node consolidation,\n",
    "        but in a way which does not generate overlapping geometries.\n",
    "        Overlapping geometries cause (unresolvable) issues with Voronoi.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network : GeoDataFrame (LineStrings)\n",
    "\n",
    "        distance : float\n",
    "            distance for interpolation\n",
    "\n",
    "        epsilon : float\n",
    "            tolerance for simplification\n",
    "\n",
    "        filter_func : function\n",
    "            function which takes gdf of polygonized network and returns mask of invalid\n",
    "            polygons (those which should be consolidated)\n",
    "\n",
    "        **kwargs\n",
    "            Additional kwargs passed to filter_func\n",
    "        \"\"\"\n",
    "\n",
    "        # polygonize network\n",
    "        polygonized = polygonize(network.geometry)\n",
    "        geoms = [g for g in polygonized]\n",
    "        gdf = gpd.GeoDataFrame(geometry=geoms, crs=network.crs)\n",
    "\n",
    "        # filter potentially incorrect polygons\n",
    "        mask = filter_func(gdf, **kwargs)\n",
    "        invalid = gdf.loc[mask]\n",
    "\n",
    "        sindex = network.sindex\n",
    "\n",
    "        # iterate over polygons which are marked to be consolidated\n",
    "        # list segments to be removed and the averaged geoms replacing them\n",
    "        averaged = []\n",
    "        to_remove = []\n",
    "        for poly in invalid.geometry:\n",
    "            real = network.iloc[sindex.query(poly.exterior, predicate=\"intersects\")]\n",
    "            mask = real.intersection(poly.exterior).type.isin(\n",
    "                [\"LineString\", \"MultiLineString\"]\n",
    "            )\n",
    "            real = real[mask]\n",
    "            lines = list(real.geometry)\n",
    "            to_remove += list(real.index)\n",
    "\n",
    "            if lines:\n",
    "                av = _average_geometry(lines, poly, distance)\n",
    "                averaged += av\n",
    "\n",
    "        # drop double lines\n",
    "        clean = network.drop(set(to_remove))\n",
    "\n",
    "        # merge new geometries with the existing network\n",
    "        averaged = gpd.array.from_shapely(averaged, crs=network.crs).simplify(epsilon)\n",
    "        result = pd.concat([clean, gpd.GeoDataFrame(geometry=averaged[~averaged.is_empty])])\n",
    "        merge = topology(result)\n",
    "\n",
    "        return merge\n",
    "\n",
    "\n",
    "    def roundabouts(gdf, area=5000, circom=0.6):\n",
    "        \"\"\"\n",
    "        Filter out roundabouts\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate parameters\n",
    "        gdf[\"area\"] = gdf.geometry.area\n",
    "        gdf[\"circom\"] = mm.CircularCompactness(gdf, \"area\").series\n",
    "        # select valid and invalid network-net_blocks\n",
    "        mask = (gdf[\"area\"] < area) & (gdf[\"circom\"] > circom)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def filter_comp(gdf, max_size=10000, circom_max=0.2):\n",
    "        \"\"\"\n",
    "        Filter based on max size and compactness\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf : GeoDataFrame\n",
    "            polygonized network\n",
    "        max_size : float\n",
    "            maximum size of a polygon to be considered potentially invalid\n",
    "        circom_max : float\n",
    "            maximum circular compactness of a polygon to be considered\n",
    "            potentially invalid.\n",
    "\n",
    "        Returns boolean series\n",
    "\n",
    "        \"\"\"\n",
    "        # calculate parameters\n",
    "        gdf[\"area\"] = gdf.geometry.area\n",
    "        gdf[\"circom\"] = mm.CircularCompactness(gdf, \"area\").series\n",
    "        # select valid and invalid network-net_blocks\n",
    "        mask = (gdf[\"area\"] < max_size) & (gdf[\"circom\"] < circom_max)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def topology(gdf):\n",
    "        \"\"\"\n",
    "        Clean topology of existing LineString geometry by removal of nodes of degree 2.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf : GeoDataFrame\n",
    "            (Multi)LineString data of street network\n",
    "        \"\"\"\n",
    "\n",
    "        # explode to avoid MultiLineStrings\n",
    "        # double reset index due to the bug in GeoPandas explode\n",
    "        df = gdf.reset_index(drop=True).explode().reset_index(drop=True)\n",
    "\n",
    "        # get underlying pygeos geometry\n",
    "        geom = df.geometry.values.data\n",
    "\n",
    "        # extract array of coordinates and number per geometry\n",
    "        coords = pygeos.get_coordinates(geom)\n",
    "        indices = pygeos.get_num_coordinates(geom)\n",
    "\n",
    "        # generate a list of start and end coordinates and create point geometries\n",
    "        edges = [0]\n",
    "        i = 0\n",
    "        for ind in indices:\n",
    "            ix = i + ind\n",
    "            edges.append(ix - 1)\n",
    "            edges.append(ix)\n",
    "            i = ix\n",
    "        edges = edges[:-1]\n",
    "        points = pygeos.points(np.unique(coords[edges], axis=0))\n",
    "\n",
    "        # query LineString geometry to identify points intersecting 2 geometries\n",
    "        tree = pygeos.STRtree(geom)\n",
    "        inp, res = tree.query_bulk(points, predicate=\"intersects\")\n",
    "        unique, counts = np.unique(inp, return_counts=True)\n",
    "        merge = res[np.isin(inp, unique[counts == 2])]\n",
    "\n",
    "        # filter duplications and create a dictionary with indication of components to be merged together\n",
    "        dups = [item for item, count in collections.Counter(merge).items() if count > 1]\n",
    "        split = np.split(merge, len(merge) / 2)\n",
    "        components = {}\n",
    "        for i, a in enumerate(split):\n",
    "            if a[0] in dups or a[1] in dups:\n",
    "                if a[0] in components.keys():\n",
    "                    i = components[a[0]]\n",
    "                elif a[1] in components.keys():\n",
    "                    i = components[a[1]]\n",
    "            components[a[0]] = i\n",
    "            components[a[1]] = i\n",
    "\n",
    "        # iterate through components and create new geometries\n",
    "        new = []\n",
    "        for c in set(components.values()):\n",
    "            keys = []\n",
    "            for item in components.items():\n",
    "                if item[1] == c:\n",
    "                    keys.append(item[0])\n",
    "            new.append(pygeos.line_merge(pygeos.union_all(geom[keys])))\n",
    "\n",
    "        # remove incorrect geometries and append fixed versions\n",
    "        df = df.drop(merge)\n",
    "        final = gpd.GeoSeries(new).explode().reset_index(drop=True)\n",
    "        return df.append(\n",
    "            gpd.GeoDataFrame({df.geometry.name: final}, geometry=df.geometry.name),\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    user = 'martin'\n",
    "    pwd = 'gdsl2020'\n",
    "    host = '138.253.73.214'\n",
    "    port = '5432'\n",
    "\n",
    "    url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\"\n",
    "    engine = create_engine(url)\n",
    "    # x, y = xy  # coordinates in epsg 27700\n",
    "    buffer = 1000  # radius in [m]\n",
    "\n",
    "    sql = f'SELECT * FROM openroads_200803_topological WHERE ST_DWithin(geometry, ST_SetSRID(ST_Point({xy[0][1:-1]}), 27700), {buffer})'\n",
    "\n",
    "    df = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "\n",
    "#     areas = range(500, 2501, 500)\n",
    "#     compactness = np.linspace(0.7, 0.9, 5)\n",
    "    areas=[500]\n",
    "    compactness=[0.8]\n",
    "    results = []\n",
    "\n",
    "    for area in areas:\n",
    "        for comp in compactness:\n",
    "            try:\n",
    "                topo = consolidate(df, filter_func=roundabouts, area=area, circom=comp)\n",
    "\n",
    "                G = mm.gdf_to_nx(topo)\n",
    "                mesh = mm.meshedness(G, radius=None)\n",
    "                G = mm.subgraph(G, meshedness=True, cds_length=False, mean_node_degree=False, proportion={0: False, 3: False, 4: False}, cyclomatic=False, edge_node_ratio=False, gamma=False, local_closeness=True, closeness_weight=None, verbose=False)\n",
    "                vals = list(nx.get_node_attributes(G, 'meshedness').values())\n",
    "                l_mesh_mean = np.mean(vals)\n",
    "                l_mesh_median = np.median(vals)\n",
    "                l_mesh_dev = np.std(vals)\n",
    "                vals = list(nx.get_node_attributes(G, 'local_closeness').values())\n",
    "                l_close_mean = np.mean(vals)\n",
    "                l_close_median = np.median(vals)\n",
    "                l_close_dev = np.std(vals)\n",
    "                node_density = nx.number_of_nodes(G) / topo.length.sum()\n",
    "\n",
    "                results += [area, comp, mesh, l_mesh_mean, l_mesh_median, l_mesh_dev, l_close_mean, l_close_median, l_close_dev]\n",
    "            except Exception:\n",
    "                results += [None, None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('https://gist.githubusercontent.com/martinfleis/4148eeb0ea19e7808e761e097a877196/raw/f3001eeb7294eb330c6675d1cab307cd562baa5c/major_cities.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## worker on the same machine as scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(cities.iloc[:28], npartitions=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=27</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Barnsley</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basildon</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colchester</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coventry</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 27 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               geometry\n",
       "npartitions=27         \n",
       "Barnsley         object\n",
       "Basildon            ...\n",
       "...                 ...\n",
       "Colchester          ...\n",
       "Coventry            ...\n",
       "Dask Name: from_pandas, 27 tasks"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ddf.apply(lambda x:  check(x), axis=1, meta=pd.Series(dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = vals.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcity15nm\n",
       "Barnsley             [500, 0.8, 0.09230769230769231, 0.083749008432...\n",
       "Basildon             [500, 0.8, 0.0608187134502924, 0.0538392863229...\n",
       "Basingstoke          [500, 0.8, 0.07091469681397738, 0.064754915482...\n",
       "Bath                 [500, 0.8, 0.1288805268109125, 0.0971790636111...\n",
       "Bedford              [500, 0.8, 0.11612903225806452, 0.086294542518...\n",
       "Birkenhead           [500, 0.8, 0.11755102040816326, 0.086451723996...\n",
       "Birmingham           [500, 0.8, 0.13355048859934854, 0.103514244459...\n",
       "Blackburn            [500, 0.8, 0.13704918032786886, 0.103527398220...\n",
       "Blackpool            [500, 0.8, 0.18566037735849056, 0.130201667412...\n",
       "Bolton               [500, 0.8, 0.13595166163141995, 0.100769073454...\n",
       "Bournemouth          [500, 0.8, 0.1659919028340081, 0.1222513564717...\n",
       "Bracknell            [500, 0.8, 0.07934893184130214, 0.072159803761...\n",
       "Bradford             [500, 0.8, 0.1520428667113195, 0.1098673229464...\n",
       "Brighton and Hove    [500, 0.8, 0.11497005988023952, 0.089599155793...\n",
       "Bristol              [500, 0.8, 0.11778290993071594, 0.082985128946...\n",
       "Burnley              [500, 0.8, 0.1543661971830986, 0.1203642447495...\n",
       "Burton upon Trent    [500, 0.8, 0.11557788944723618, 0.088822890608...\n",
       "Bury                 [500, 0.8, 0.15895710681244743, 0.131526511189...\n",
       "Cambridge            [500, 0.8, 0.10031347962382445, 0.084637419572...\n",
       "Cardiff              [500, 0.8, 0.18863456985003946, 0.144604843220...\n",
       "Carlisle             [500, 0.8, 0.15940685820203893, 0.124386760887...\n",
       "Chatham              [500, 0.8, 0.04103967168262654, 0.035790955311...\n",
       "Chelmsford           [500, 0.8, 0.10414452709883103, 0.077870383089...\n",
       "Cheltenham           [500, 0.8, 0.12415349887133183, 0.089047400472...\n",
       "Chester              [500, 0.8, 0.10142497904442582, 0.074611635206...\n",
       "Chesterfield         [500, 0.8, 0.11030176899063475, 0.085345033632...\n",
       "Colchester           [500, 0.8, 0.11749680715197956, 0.088470523397...\n",
       "Coventry             [500, 0.8, 0.1203281677301732, 0.0873586876332...\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workers on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8787/status' target='_blank'>http://172.17.0.2:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>56</li>\n",
       "  <li><b>Memory: </b>151.64 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:8786' processes=2 threads=56, memory=151.64 GB>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(cities.iloc[:28], npartitions=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=27</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Barnsley</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basildon</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colchester</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coventry</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 27 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               geometry\n",
       "npartitions=27         \n",
       "Barnsley         object\n",
       "Basildon            ...\n",
       "...                 ...\n",
       "Colchester          ...\n",
       "Coventry            ...\n",
       "Dask Name: from_pandas, 27 tasks"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ddf.apply(lambda x:  check(x), axis=1, meta=pd.Series(dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = vals.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcity15nm\n",
       "Barnsley             [500, 0.8, 0.09230769230769231, 0.083749008432...\n",
       "Basildon             [500, 0.8, 0.0608187134502924, 0.0538392863229...\n",
       "Basingstoke          [500, 0.8, 0.07091469681397738, 0.064754915482...\n",
       "Bath                 [500, 0.8, 0.1288805268109125, 0.0971790636111...\n",
       "Bedford              [500, 0.8, 0.11612903225806452, 0.086294542518...\n",
       "Birkenhead           [500, 0.8, 0.11755102040816326, 0.086451723996...\n",
       "Birmingham           [500, 0.8, 0.13355048859934854, 0.103514244459...\n",
       "Blackburn            [500, 0.8, 0.13704918032786886, 0.103527398220...\n",
       "Blackpool            [500, 0.8, 0.18566037735849056, 0.130201667412...\n",
       "Bolton               [500, 0.8, 0.13595166163141995, 0.100769073454...\n",
       "Bournemouth          [500, 0.8, 0.1659919028340081, 0.1222513564717...\n",
       "Bracknell            [500, 0.8, 0.07934893184130214, 0.072159803761...\n",
       "Bradford             [500, 0.8, 0.1520428667113195, 0.1098673229464...\n",
       "Brighton and Hove    [500, 0.8, 0.11497005988023952, 0.089599155793...\n",
       "Bristol              [500, 0.8, 0.11778290993071594, 0.082985128946...\n",
       "Burnley              [500, 0.8, 0.1543661971830986, 0.1203642447495...\n",
       "Burton upon Trent    [500, 0.8, 0.11557788944723618, 0.088822890608...\n",
       "Bury                 [500, 0.8, 0.15895710681244743, 0.131526511189...\n",
       "Cambridge            [500, 0.8, 0.10031347962382445, 0.084637419572...\n",
       "Cardiff              [500, 0.8, 0.18863456985003946, 0.144604843220...\n",
       "Carlisle             [500, 0.8, 0.15940685820203893, 0.124386760887...\n",
       "Chatham              [500, 0.8, 0.04103967168262654, 0.035790955311...\n",
       "Chelmsford           [500, 0.8, 0.10414452709883103, 0.077870383089...\n",
       "Cheltenham           [500, 0.8, 0.12415349887133183, 0.089047400472...\n",
       "Chester              [500, 0.8, 0.10142497904442582, 0.074611635206...\n",
       "Chesterfield         [500, 0.8, 0.11030176899063475, 0.085345033632...\n",
       "Colchester           [500, 0.8, 0.11749680715197956, 0.088470523397...\n",
       "Coventry             [500, 0.8, 0.1203281677301732, 0.0873586876332...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less workers per machine makes it much faster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(cities, npartitions=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=28</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Barnsley</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bedford</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wolverhampton</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>York</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 28 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               geometry\n",
       "npartitions=28         \n",
       "Barnsley         object\n",
       "Bedford             ...\n",
       "...                 ...\n",
       "Wolverhampton       ...\n",
       "York                ...\n",
       "Dask Name: from_pandas, 28 tasks"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ddf.apply(lambda x:  check(x), axis=1, meta=pd.Series(dtype='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = vals.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tcity15nm\n",
       "Barnsley         [500, 0.8, 0.09230769230769231, 0.083749008432...\n",
       "Basildon         [500, 0.8, 0.0608187134502924, 0.0538392863229...\n",
       "Basingstoke      [500, 0.8, 0.07091469681397738, 0.064754915482...\n",
       "Bath             [500, 0.8, 0.1288805268109125, 0.0971790636111...\n",
       "Bedford          [500, 0.8, 0.11612903225806452, 0.086294542518...\n",
       "                                       ...                        \n",
       "Woking           [500, 0.8, 0.08414239482200647, 0.062861526938...\n",
       "Wolverhampton    [500, 0.8, 0.11099020674646355, 0.080441009150...\n",
       "Worcester        [500, 0.8, 0.0926130099228225, 0.0709719598774...\n",
       "Worthing         [500, 0.8, 0.07547169811320754, 0.056878655718...\n",
       "York             [500, 0.8, 0.10531220876048462, 0.089765475222...\n",
       "Length: 112, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
